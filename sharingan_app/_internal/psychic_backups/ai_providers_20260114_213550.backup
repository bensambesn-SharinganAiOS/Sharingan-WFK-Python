{
  "capability_name": "ai_providers",
  "content": "#!/usr/bin/env python3\n\"\"\"\nAI Providers System - Hybrid routing with fallback\nCombines tgpt (fast, free) + MiniMax (free API) with intelligent routing\n\"\"\"\n\nimport subprocess\nimport json\nimport time\nimport os\nimport hashlib\nimport shutil\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Any\nfrom dataclasses import dataclass, field, asdict\nfrom pathlib import Path\nimport logging\nimport threading\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"sharingan.providers\")\n\n@dataclass\nclass ModelMetrics:\n    \"\"\"Performance metrics for a model\"\"\"\n    model_name: str\n    provider: str\n    total_calls: int = 0\n    successful_calls: int = 0\n    failed_calls: int = 0\n    total_tokens: int = 0\n    total_time_ms: float = 0.0\n    avg_response_time_ms: float = 0.0\n    last_success: Optional[str] = None\n    last_failure: Optional[str] = None\n    error_types: Dict[str, int] = field(default_factory=dict)\n    \n    def record_call(self, success: bool, tokens: int, time_ms: float, error: Optional[str] = None):\n        self.total_calls += 1\n        self.total_time_ms += time_ms\n        self.total_tokens += tokens\n        if self.total_calls > 0:\n            self.avg_response_time_ms = self.total_time_ms / self.total_calls\n        \n        if success:\n            self.successful_calls += 1\n            self.last_success = datetime.now().isoformat()\n        else:\n            self.failed_calls += 1\n            self.last_failure = datetime.now().isoformat()\n            if error:\n                self.error_types[error] = self.error_types.get(error, 0) + 1\n    \n    def success_rate(self) -> float:\n        if self.total_calls == 0:\n            return 0.0\n        return (self.successful_calls / self.total_calls) * 100\n    \n    def to_dict(self) -> Dict:\n        return asdict(self)\n\nclass AIProvider:\n    \"\"\"Base class for AI providers\"\"\"\n    \n    def __init__(self, name: str, model: str, api_key: Optional[str] = None):\n        self.name = name\n        self.model = model\n        self.api_key = api_key\n        self.metrics = ModelMetrics(model_name=model, provider=name)\n        self.available = False\n    \n    def is_available(self) -> bool:\n        return self.available\n    \n    def chat(self, message: str, context: Optional[List[Dict]] = None) -> Dict:\n        \"\"\"Send chat message and get response\"\"\"\n        start = time.time()\n        try:\n            result = self._execute_chat(message, context)\n            elapsed = (time.time() - start) * 1000\n            tokens = len(result.get(\"response\", \"\").split()) * 4\n            self.metrics.record_call(True, tokens, elapsed)\n            return {\n                \"success\": True,\n                \"response\": result.get(\"response\", \"\"),\n                \"model\": self.model,\n                \"provider\": self.name,\n                \"time_ms\": elapsed\n            }\n        except Exception as e:\n            elapsed = (time.time() - start) * 1000\n            self.metrics.record_call(False, 0, elapsed, error=str(e))\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"model\": self.model,\n                \"provider\": self.name\n            }\n    \n    def _execute_chat(self, message: str, context: Optional[List[Dict]] = None) -> Dict:\n        raise NotImplementedError\n    \n    def get_metrics(self) -> Dict:\n        return self.metrics.to_dict()\n\nclass TgptProvider(AIProvider):\n    \"\"\"tgpt - CLI wrapper for Phind/Grok API (primary, fast, free)\"\"\"\n    \n    def __init__(self):\n        super().__init__(\"tgpt\", \"phind-grok\")\n        self._check_availability()\n    \n    def _check_availability(self):\n        self.available = shutil.which(\"tgpt\") is not None\n    \n    def _execute_chat(self, message: str, context: Optional[List[Dict]] = None) -> Dict:\n        full_message = message\n        if context:\n            context_parts = []\n            for msg in context[-5:]:\n                role = msg.get(\"role\", \"user\")\n                content = msg.get(\"content\", \"\")[:300]\n                context_parts.append(f\"[{role}]: {content}\")\n            if context_parts:\n                context_str = \"\\n\".join(reversed(context_parts))\n                full_message = f\"CONTEXTE PR\u00c9C\u00c9DENT:\\n{context_str}\\n\\nQUESTION ACTUELLE: {message}\"\n        \n        cmd = [\"tgpt\", \"--model\", \"grok\", \"--quiet\", full_message]\n        \n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=60\n        )\n        \n        if result.returncode != 0:\n            raise RuntimeError(f\"tgpt failed: {result.stderr}\")\n        \n        return {\"response\": result.stdout.strip()}\n\nclass MiniMaxProvider(AIProvider):\n    \"\"\"MiniMax - Free API (backup, good for coding)\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        super().__init__(\"minimax\", \"MiniMax-m2.1-free\", api_key)\n        self.api_base = \"https://api.minimax.chat/v1/text/chatcompletion_v2\"\n        self.available = True\n    \n    def _execute_chat(self, message: str, context: Optional[List[Dict]] = None) -> Dict:\n        import urllib.request\n        import urllib.error\n        \n        api_key = self.api_key or os.getenv(\"MINIMAX_API_KEY\", \"\")\n        \n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n        \n        messages = [{\"role\": \"user\", \"content\": message}]\n        if context:\n            for msg in context[-5:]:\n                messages.insert(0, {\"role\": msg.get(\"role\", \"user\"), \"content\": msg.get(\"content\", \"\")[:500]})\n        \n        payload = {\n            \"model\": \"abab6.5s-chat\",\n            \"messages\": messages,\n            \"temperature\": 0.7,\n            \"max_tokens\": 4096\n        }\n        \n        try:\n            req = urllib.request.Request(\n                self.api_base,\n                data=json.dumps(payload).encode(),\n                headers=headers,\n                method=\"POST\"\n            )\n            \n            with urllib.request.urlopen(req, timeout=30) as response:\n                data = json.loads(response.read().decode())\n                choice = data.get(\"choices\", [{}])[0]\n                return {\"response\": choice.get(\"message\", {}).get(\"content\", \"\")}\n        except urllib.error.HTTPError as e:\n            error_body = e.read().decode()\n            logger.warning(f\"MiniMax HTTP {e.code}: {error_body[:200]}\")\n            raise RuntimeError(f\"MiniMax API error: {error_body[:200]}\")\n\n\nclass GrokCodeFastProvider(AIProvider):\n    \"\"\"Grok Code Fast 1 - Free model from OpenCode Zen (fast coding)\"\"\"\n    \n    def __init__(self):\n        super().__init__(\"grok-code-fast\", \"grok-code-fast-1\")\n        self._check_availability()\n    \n    def _check_availability(self):\n        self.available = shutil.which(\"tgpt\") is not None\n    \n    def _execute_chat(self, message: str, context: Optional[List[Dict]] = None) -> Dict:\n        full_message = message\n        if context:\n            context_parts = []\n            for msg in context[-5:]:\n                role = msg.get(\"role\", \"user\")\n                content = msg.get(\"content\", \"\")[:300]\n                context_parts.append(f\"[{role}]: {content}\")\n            if context_parts:\n                context_str = \"\\n\".join(reversed(context_parts))\n                full_message = f\"CONTEXTE PR\u00c9C\u00c9DENT:\\n{context_str}\\n\\nQUESTION ACTUELLE: {message}\"\n        \n        cmd = [\"tgpt\", \"--model\", \"grok-code-fast\", \"--quiet\", full_message]\n        \n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=60\n        )\n        \n        if result.returncode != 0:\n            raise RuntimeError(f\"Grok Code Fast failed: {result.stderr}\")\n        \n        return {\"response\": result.stdout.strip()}\n\n\nclass HybridProviderManager:\n    \"\"\"\n    Hybrid routing system combining tgpt + MiniMax + Grok Code Fast\n    Strategy: Option 3 (routing) + Option 4 (fallback)\n    \n    3 Free Models:\n    - tgpt: Phind/Grok (general purpose, fast)\n    - minimax: MiniMax API (coding, analysis)\n    - grok-code-fast: Grok Code Fast 1 (coding specialist)\n    \"\"\"\n    \n    ROUTING_RULES = {\n        \"fast\": [\"tgpt\"],                    # Quick questions \u2192 tgpt\n        \"coding\": [\"grok-code-fast\", \"tgpt\", \"minimax\"],  # Code \u2192 Grok Fast first\n        \"analysis\": [\"tgpt\", \"minimax\"],     # Analysis \u2192 tgpt + MiniMax\n        \"creative\": [\"minimax\", \"tgpt\"],     # Creative \u2192 MiniMax\n        \"default\": [\"tgpt\", \"grok-code-fast\", \"minimax\"]  # Default \u2192 all in order\n    }\n    \n    def __init__(self):\n        self.providers: Dict[str, AIProvider] = {}\n        self.fallback_order = [\"tgpt\", \"grok-code-fast\", \"minimax\"]\n        self.current_strategy = \"default\"\n        self._initialize_providers()\n        self.lock = threading.Lock()\n        self.parallel_results: List[Dict] = []\n    \n    def _initialize_providers(self):\n        \"\"\"Initialize available providers\"\"\"\n        # tgpt - Phind/Grok (primary, general purpose)\n        tgpt = TgptProvider()\n        if tgpt.is_available():\n            self.providers[\"tgpt\"] = tgpt\n            logger.info(\"tgpt provider initialized (primary - Phind/Grok)\")\n        \n        # Grok Code Fast 1 - Coding specialist\n        grok_fast = GrokCodeFastProvider()\n        if grok_fast.is_available():\n            self.providers[\"grok-code-fast\"] = grok_fast\n            logger.info(\"Grok Code Fast 1 initialized (coding specialist)\")\n        \n        # MiniMax - API backup (good for coding/analysis)\n        minimax = MiniMaxProvider()\n        if minimax.is_available():\n            self.providers[\"minimax\"] = minimax\n            logger.info(\"MiniMax provider initialized (API backup)\")\n    \n    def detect_query_type(self, query: str) -> str:\n        \"\"\"Detect query type for intelligent routing\"\"\"\n        q = query.lower()\n        \n        # Code-related queries\n        if any(x in q for x in [\"code\", \"function\", \"class\", \"python\", \"javascript\", \n                                 \"debug\", \"error\", \"syntax\", \"api\", \"implement\"]):\n            return \"coding\"\n        \n        # Analysis/Research queries\n        if any(x in q for x in [\"analyze\", \"explain\", \"compare\", \"why\", \"how works\",\n                                 \"difference\", \"advantages\", \"disadvantages\"]):\n            return \"analysis\"\n        \n        # Creative queries\n        if any(x in q for x in [\"write\", \"create\", \"generate\", \"story\", \" poem\",\n                                 \"design\", \"creative\"]):\n            return \"creative\"\n        \n        # Quick questions\n        if any(x in q for x in [\"what is\", \"who is\", \"define\", \"list\"]):\n            return \"fast\"\n        \n        return \"default\"\n    \n    def get_providers_for_query(self, query: str) -> List[str]:\n        \"\"\"Get ordered list of providers for a query\"\"\"\n        query_type = self.detect_query_type(query)\n        return self.ROUTING_RULES.get(query_type, self.ROUTING_RULES[\"default\"])\n    \n    def chat_single(self, message: str, provider: str, \n                   context: Optional[List[Dict]] = None) -> Dict:\n        \"\"\"Chat with a single provider\"\"\"\n        if provider not in self.providers:\n            return {\"success\": False, \"error\": f\"Provider {provider} not available\"}\n        \n        return self.providers[provider].chat(message, context)\n    \n    def chat_parallel(self, message: str, providers: List[str],\n                     context: Optional[List[Dict]] = None,\n                     timeout_ms: int = 30000) -> List[Dict]:\n        \"\"\"Execute multiple providers in parallel and return all results\"\"\"\n        results = []\n        start = time.time()\n        \n        def call_provider(p):\n            return self.providers[p].chat(message, context)\n        \n        threads = []\n        for provider in providers:\n            if provider in self.providers:\n                t = threading.Thread(target=lambda: results.append(call_provider(provider)))\n                t.start()\n                threads.append(t)\n        \n        for t in threads:\n            t.join(timeout=timeout_ms / 1000)\n        \n        return results\n    \n    def chat_hybrid(self, message: str, \n                   context: Optional[List[Dict]] = None,\n                   mode: str = \"auto\") -> Dict:\n        \"\"\"\n        Hybrid chat with intelligent routing + fallback\n        \n        Modes:\n        - \"auto\": Auto-detect query type and route accordingly\n        - \"fast\": tgpt only (quickest)\n        - \"parallel\": Both providers, return best\n        - \"fallback\": tgpt first, fallback to minimax\n        - \"coding\": Both for code, compare results\n        \"\"\"\n        query_type = self.detect_query_type(message)\n        providers = self.get_providers_for_query(message)\n        \n        logger.info(f\"Query type: {query_type}, providers: {providers}\")\n        \n        if mode == \"fast\":\n            # tgpt only (fastest)\n            result = self.chat_single(message, \"tgpt\", context)\n            result[\"strategy\"] = \"fast\"\n            result[\"providers_used\"] = [\"tgpt\"]\n            return result\n        \n        elif mode == \"parallel\":\n            # Both in parallel, return all results\n            results = self.chat_parallel(message, providers, context)\n            successful = [r for r in results if r.get(\"success\")]\n            \n            if not successful:\n                return {\"success\": False, \"error\": \"All providers failed\"}\n            \n            # Return best (fastest successful)\n            best = min(successful, key=lambda x: x.get(\"time_ms\", 99999))\n            best[\"strategy\"] = \"parallel\"\n            best[\"all_results\"] = results\n            best[\"providers_used\"] = providers\n            return best\n        \n        elif mode == \"coding\":\n            # Both for code, compare and return best quality\n            results = self.chat_parallel(message, [\"tgpt\", \"minimax\"], context)\n            successful = [r for r in results if r.get(\"success\")]\n            \n            if not successful:\n                return {\"success\": False, \"error\": \"All providers failed\"}\n            \n            # For coding, MiniMax often better - prioritize it\n            minimax_result = next((r for r in successful if r.get(\"provider\") == \"minimax\"), None)\n            tgpt_result = next((r for r in successful if r.get(\"provider\") == \"tgpt\"), None)\n            \n            if minimax_result:\n                minimax_result[\"strategy\"] = \"coding\"\n                minimax_result[\"comparison\"] = {\n                    \"tgpt_time\": tgpt_result.get(\"time_ms\") if tgpt_result else None,\n                    \"minimax_time\": minimax_result.get(\"time_ms\")\n                }\n                minimax_result[\"providers_used\"] = [\"tgpt\", \"minimax\"]\n                return minimax_result\n            \n            return successful[0]\n        \n        else:  # fallback (default)\n            # Try tgpt first, fallback to minimax\n            result = self.chat_single(message, \"tgpt\", context)\n            \n            if result.get(\"success\"):\n                result[\"strategy\"] = \"fallback_primary\"\n                result[\"providers_used\"] = [\"tgpt\"]\n                return result\n            \n            # Fallback to minimax\n            logger.info(\"tgpt failed, trying minimax...\")\n            result = self.chat_single(message, \"minimax\", context)\n            result[\"strategy\"] = \"fallback_backup\"\n            result[\"providers_used\"] = [\"tgpt\", \"minimax\"]\n            return result\n    \n    def chat(self, message: str, provider: Optional[str] = None,\n             context: Optional[List[Dict]] = None,\n             mode: str = \"auto\") -> Dict:\n        \"\"\"\n        Main chat interface\n        \n        Args:\n            message: The message to send\n            provider: Specific provider (overrides auto-detection)\n            context: Conversation history\n            mode: auto/fast/parallel/fallback/coding\n        \n        Returns:\n            Dict with success, response, provider, time_ms, strategy\n        \"\"\"\n        if provider:\n            # Specific provider requested\n            result = self.chat_single(message, provider, context)\n            result[\"strategy\"] = \"specific\"\n            result[\"providers_used\"] = [provider]\n            return result\n        \n        # Use hybrid routing\n        return self.chat_hybrid(message, context, mode)\n    \n    def get_metrics(self) -> Dict:\n        \"\"\"Get metrics for all providers\"\"\"\n        return {\n            name: provider.get_metrics()\n            for name, provider in self.providers.items()\n        }\n    \n    def get_status(self) -> Dict:\n        \"\"\"Get system status\"\"\"\n        return {\n            \"providers\": {\n                name: {\n                    \"available\": p.is_available(),\n                    \"model\": p.model,\n                    \"calls\": p.metrics.total_calls,\n                    \"success_rate\": p.metrics.success_rate(),\n                    \"avg_time_ms\": p.metrics.avg_response_time_ms\n                }\n                for name, p in self.providers.items()\n            },\n            \"default_strategy\": self.current_strategy,\n            \"routing_rules\": self.ROUTING_RULES\n        }\n\n\ndef get_provider_manager() -> HybridProviderManager:\n    \"\"\"Get provider manager singleton\"\"\"\n    return HybridProviderManager()\n\n\ndef ai_chat(message: str, provider: Optional[str] = None,\n            context: Optional[List[Dict]] = None,\n            mode: str = \"auto\") -> Dict:\n    \"\"\"Convenience function for AI chat\"\"\"\n    manager = get_provider_manager()\n    return manager.chat(message, provider, context, mode)\n\n\nif __name__ == \"__main__\":\n    print(\"=== HYBRID AI PROVIDERS TEST ===\\n\")\n    \n    manager = get_provider_manager()\n    \n    print(\"Status:\")\n    status = manager.get_status()\n    print(json.dumps(status, indent=2))\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    test_queries = [\n        (\"Quick question\", \"fast\"),\n        (\"Write a Python function to calculate factorial\", \"coding\"),\n        (\"Explain the difference between TCP and UDP\", \"analysis\"),\n        (\"Create a creative story about AI\", \"creative\"),\n    ]\n    \n    for query, mode in test_queries:\n        print(f\"\\nQuery: {query[:50]}...\")\n        print(f\"Mode: {mode}\")\n        \n        result = manager.chat(query, mode=mode)\n        \n        if result.get(\"success\"):\n            print(f\"  Provider: {result.get('provider')}\")\n            print(f\"  Strategy: {result.get('strategy')}\")\n            print(f\"  Time: {result.get('time_ms', 0):.0f}ms\")\n            print(f\"  Response: {result.get('response', '')[:100]}...\")\n        else:\n            print(f\"  Error: {result.get('error')}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Metrics:\")\n    metrics = manager.get_metrics()\n    print(json.dumps(metrics, indent=2))\n",
  "checksum": "9b2d195adc6a277ac63e73adc8a428abc1f921ee7c7d894747d8836cb018fd0f",
  "created_at": "2026-01-14T21:35:50.152774",
  "version": "1.0"
}